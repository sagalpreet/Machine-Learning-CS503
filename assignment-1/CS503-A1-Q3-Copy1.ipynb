{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b1d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, floor\n",
    "import random\n",
    "import csv\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b13c92",
   "metadata": {},
   "source": [
    "## Linear Ridge Regression in Multiple Dimension [Medium]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883981bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for creating a dataset\n",
    "\n",
    "### data structure to represent dataset\n",
    "class Dataset:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, n = None):\n",
    "        # todo: assertion to verify the dimension of x and y\n",
    "        self.__x = x\n",
    "        self.__y = y\n",
    "        self.__n = n if n else len(x)\n",
    "        \n",
    "    def split(self, split_fraction):\n",
    "        # splits the dataset into two parts\n",
    "        X1, Y1 = [], []\n",
    "        X2, Y2 = [], []\n",
    "        for x, y in dataset:\n",
    "            if (random.random() < split_fraction):\n",
    "                X1.append(x)\n",
    "                Y1.append(y)\n",
    "            else:\n",
    "                X2.append(x)\n",
    "                Y2.append(y)\n",
    "        d1 = Dataset(np.array(X1), np.array(Y1))\n",
    "        d2 = Dataset(np.array(X2), np.array(Y2))\n",
    "        return d1, d2\n",
    "    \n",
    "    def k_split(self, k):\n",
    "        if (k < 2):\n",
    "            raise AssertionError('Cannot split dataset into 1')\n",
    "            \n",
    "        batch_size = ceil(self.__n / k)\n",
    "        x, y = list(self.__x), list(self.__y)\n",
    "        \n",
    "        for i in range(k):\n",
    "            x_temp = x[:batch_size*i] + x[batch_size*(i+1):]\n",
    "            y_temp = y[:batch_size*i] + y[batch_size*(i+1):]\n",
    "            train = Dataset(np.array(x_temp), np.array(y_temp))\n",
    "            \n",
    "            x_temp = x[batch_size*i : batch_size*(i+1)]\n",
    "            y_temp = y[batch_size*i : batch_size*(i+1)]\n",
    "            validation = Dataset(np.array(x_temp), np.array(y_temp))\n",
    "            \n",
    "            yield (train, validation)\n",
    "    \n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.__x\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.__y\n",
    "    \n",
    "    @x.setter\n",
    "    def x(self, value):\n",
    "        self.__x = value\n",
    "        \n",
    "    @y.setter\n",
    "    def y(self, value):\n",
    "        self.__y = value\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        #todo: assertion to verify out of bounds\n",
    "        return self.__x[index], self.__y[index]\n",
    "    \n",
    "    def __setitem__(self, index: int, x_: np.ndarray, y_: np.ndarray):\n",
    "        # todo: assertion to verify out of bounds\n",
    "        self.__x[index] = x_\n",
    "        self.__y[index] = y_\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.__n\n",
    "    \n",
    "    def __del__(self):\n",
    "        del(self.__x)\n",
    "        del(self.__y)\n",
    "        del(self.__n)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.__index = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if (self.__index < self.__n):\n",
    "            self.__index += 1\n",
    "            return self[self.__index - 1]\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0aa5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and create a dataset out of it\n",
    "# NOTE: The only label field must be the last column\n",
    "\n",
    "def read_raw(path: str):\n",
    "    # read raw data from csv\n",
    "    # convert str to float\n",
    "    # for every field possible\n",
    "    file = open(path, \"r\")\n",
    "    raw_data = csv.reader(file, delimiter = ',')\n",
    "    \n",
    "    data = []\n",
    "    for row in raw_data:\n",
    "        for (i, value) in enumerate(row):\n",
    "            try:\n",
    "                row[i] = float(value)\n",
    "            except:\n",
    "                pass\n",
    "        data.append(row)\n",
    "    file.close()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_field_info(data):\n",
    "    num_rows, num_cols = len(data), len(data[0])\n",
    "    \n",
    "    # extract information about numeric and non-numeric fields\n",
    "    non_numeric_fields = {}\n",
    "    numeric_fields = set([])\n",
    "    \n",
    "    for index in range(num_cols-1):\n",
    "        if type(data[0][index]) == float:\n",
    "            numeric_fields.add(index)\n",
    "            continue\n",
    "        # for each non-numeric field, we maintain information about number\n",
    "        # and types of different values possible for that field\n",
    "        non_numeric_fields[index] = {'count': -1, 'values': {}}\n",
    "    \n",
    "    for row in data:\n",
    "        for index, field in non_numeric_fields.items():\n",
    "            value = row[index]\n",
    "            if (value not in field['values']):\n",
    "                field['count'] += 1\n",
    "                field['values'][value] = field['count']\n",
    "    \n",
    "    return non_numeric_fields, numeric_fields\n",
    "\n",
    "def construct_design_matrix(data, non_numeric_fields, numeric_fields):\n",
    "    num_rows, num_cols = len(data), len(data[0])\n",
    "    \n",
    "    # constructing desired design matrix and label vector.\n",
    "    # we encode non-numeric values using one-hot encoding.\n",
    "    # also we add \"1\" to every feature vector to accomodate constant\n",
    "    \n",
    "    # however, after one hot encoding, we eliminate a column\n",
    "    # for each original non-numeric field to reduce correlation\n",
    "    # between newly formed fields\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i, row in enumerate(data):\n",
    "        x = []\n",
    "        Y.append(row[num_cols-1])\n",
    "        for index in range(num_cols - 1):\n",
    "            \n",
    "            value = row[index]\n",
    "            \n",
    "            # append numeric feature as it is\n",
    "            if (index in numeric_fields):\n",
    "                x.append(row[index])\n",
    "                continue\n",
    "                \n",
    "            # encode non-numeric feature and append\n",
    "            field = non_numeric_fields[index]\n",
    "            one_hot_encoded = [0]*field['count']\n",
    "            pos = field['values'][value]\n",
    "            if (pos): one_hot_encoded[pos-1] = 1\n",
    "            x.extend(one_hot_encoded)\n",
    "        \n",
    "        X.append(x)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def get_dataset(path: str, hasHeader = True):\n",
    "    \n",
    "    data = read_raw(path)\n",
    "    \n",
    "    # remove first row if it is a header\n",
    "    if (hasHeader):\n",
    "        data = data[1:]\n",
    "    \n",
    "    # check if there is data\n",
    "    if (not data):\n",
    "        raise IndexError('No data in the given file')\n",
    "    \n",
    "    # extract information about numeric and non-numeric fields\n",
    "    non_numeric_fields, numeric_fields = get_field_info(data)\n",
    "    \n",
    "    X, Y = construct_design_matrix(data, non_numeric_fields, numeric_fields)\n",
    "    \n",
    "    return Dataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fea871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up path of .csv file\n",
    "path = 'dataset/insurance.csv'\n",
    "\n",
    "dataset = get_dataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfadd23",
   "metadata": {},
   "source": [
    "#### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd5cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Mean and Variance for Normalized Data (for each feature):\n",
      "Mean\t\t[[-0. -0.  0.  0.  0.  0.  0. -0.]]\n",
      "Variance\t[[1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# normalize a 2-d matrix\n",
    "\n",
    "def get_mean_variance(matrix):\n",
    "    num_rows, num_cols = matrix.shape\n",
    "    mean = np.sum(matrix, axis=0, keepdims=True) / num_rows\n",
    "    variance = (np.sum((matrix - mean) ** 2, axis=0, keepdims=True) / (num_rows))\n",
    "    return mean, variance\n",
    "\n",
    "def normalize(matrix, mean, variance):\n",
    "    num_rows, num_cols = matrix.shape\n",
    "    std = variance ** 0.5\n",
    "    for i in range(num_cols):\n",
    "        if (std[0][i] == 0):\n",
    "            std[0][i] = 1\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "dataset.x = normalize(dataset.x, *get_mean_variance(dataset.x))\n",
    "mean, variance = get_mean_variance(dataset.x)\n",
    "print(f'Verifying Mean and Variance for Normalized Data (for each feature):\\nMean\\t\\t{np.around(mean, 4)}\\nVariance\\t{np.around(variance, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46917a40",
   "metadata": {},
   "source": [
    "#### Partition dataset into (training and validation) and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac74c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_validation_set = dataset.split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cac5f8",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4370129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent for ridge regression\n",
    "# and k_cross_validation\n",
    "\n",
    "### setting up hyperparameters\n",
    "num_iterations = int(1e5)\n",
    "learning_rate = 1e-8\n",
    "\n",
    "def ridgereg(X, Y, lamda, num_iterations = num_iterations):\n",
    "    m = np.array([1]*X.shape[0])\n",
    "    w, b = np.random.rand(X.shape[1]), np.random.random()\n",
    "    \n",
    "    # gradient descent updates\n",
    "    for iteration in range(1, num_iterations+1):\n",
    "        y = predridge(X, w, b)\n",
    "        temp = X@w\n",
    "        b -= learning_rate * ((m.T @ temp) + b - (Y.T @ m))\n",
    "        w -= learning_rate * ((X.T @ (temp + b*m - Y)) + lamda * w)\n",
    "        \n",
    "        if (iteration % 1000 == 0):\n",
    "            delta = predridge(X, w, b) - Y\n",
    "            loss = np.sum(delta*delta) + np.sum(lamda * (w * w))\n",
    "            print(f'Training error on {iteration}th iteration  ==> {loss}\\t\\t', end = '\\r')\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    y = predridge(X, w, b)\n",
    "    train_error = np.sum((Y - y) ** 2)\n",
    "\n",
    "    return w, b, train_error\n",
    "\n",
    "def predridge(X, w, b):\n",
    "    m = np.array([1]*X.shape[0])\n",
    "    return (X@w) + b*m\n",
    "\n",
    "\n",
    "def k_cross_validation(dataset, k, lamda):\n",
    "    kcv_datasets = dataset.k_split(k)\n",
    "    \n",
    "    validation_errors = []\n",
    "    train_errors = []\n",
    "    \n",
    "    for train, validation in kcv_datasets:\n",
    "        w, b, train_error = ridgereg(train.x, train.y, lamda)\n",
    "        y = predridge(validation.x, w, b)\n",
    "        validation_error = np.sum((validation.y - y) ** 2)\n",
    "        print(f'Validation error ==> {validation_error}')\n",
    "        validation_errors.append(validation_error)\n",
    "        train_errors.append(train_error)\n",
    "    \n",
    "    return train_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f81506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error on 100000th iteration  ==> 86094334020.2541\t\t\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2663.69748747,   213.37437664,  1554.96899031,   513.7201804 ,\n",
       "        -7034.26090618,   215.71472547,   -24.82559425,   205.72745958]),\n",
       " 17747.874047512345,\n",
       " 86094334020.2541)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgereg(dataset.x, dataset.y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c182cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0\n",
      "Training error on 100000th iteration  ==> 65187292088.38606\t\t\n",
      "Validation error ==> 7549440242.123717\n",
      "Training error on 100000th iteration  ==> 64835346249.07551\t\t\n",
      "Validation error ==> 7358286469.60691\n",
      "Training error on 100000th iteration  ==> 64805510960.47504\t\t\n",
      "Validation error ==> 6656706229.939175\n",
      "Training error on 100000th iteration  ==> 64767003189.778885\t\t\n",
      "Validation error ==> 6799212558.562197\n",
      "Training error on 100000th iteration  ==> 64161228963.57832\t\t\n",
      "Validation error ==> 8590777239.465034\n",
      "Training error on 100000th iteration  ==> 66121137492.73205\t\t\n",
      "Validation error ==> 6498941262.573219\n",
      "Training error on 100000th iteration  ==> 65709356406.794266\t\t\n",
      "Validation error ==> 7190511577.959074\n",
      "Training error on 100000th iteration  ==> 63668458382.26724\t\t\n",
      "Validation error ==> 7572981407.70616\n",
      "Training error on 100000th iteration  ==> 65154025029.87246\t\t\n",
      "Validation error ==> 6390506092.64352\n",
      "Training error on 100000th iteration  ==> 64632498004.56526\t\t\n",
      "Validation error ==> 8111354466.689083\n",
      "=======\n",
      "\n",
      "lambda: 1e-05\n",
      "Training error on 100000th iteration  ==> 65187796846.54193\t\t\n",
      "Validation error ==> 7549493980.051697\n",
      "Training error on 100000th iteration  ==> 64838776352.346016\t\t\n",
      "Validation error ==> 7358707344.659847\n",
      "Training error on 99000th iteration  ==> 64098920744.90393\t\t\t\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lamda \u001b[38;5;129;01min\u001b[39;00m lambdas:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlamda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     results[lamda] \u001b[38;5;241m=\u001b[39m \u001b[43mk_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=======\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m### plotting results\u001b[39;00m\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mk_cross_validation\u001b[0;34m(dataset, k, lamda)\u001b[0m\n\u001b[1;32m     40\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, validation \u001b[38;5;129;01min\u001b[39;00m kcv_datasets:\n\u001b[0;32m---> 43\u001b[0m     w, b, train_error \u001b[38;5;241m=\u001b[39m \u001b[43mridgereg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     y \u001b[38;5;241m=\u001b[39m predridge(validation\u001b[38;5;241m.\u001b[39mx, w, b)\n\u001b[1;32m     45\u001b[0m     validation_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((validation\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mridgereg\u001b[0;34m(X, Y, lamda, num_iterations)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     y \u001b[38;5;241m=\u001b[39m predridge(X, w, b)\n\u001b[0;32m---> 15\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;129;43m@w\u001b[39;49m\n\u001b[1;32m     16\u001b[0m     b \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m ((m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m temp) \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m-\u001b[39m (Y\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m m))\n\u001b[1;32m     17\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m ((X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (temp \u001b[38;5;241m+\u001b[39m b\u001b[38;5;241m*\u001b[39mm \u001b[38;5;241m-\u001b[39m Y)) \u001b[38;5;241m+\u001b[39m lamda \u001b[38;5;241m*\u001b[39m w)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for multiple lambda values, we calculate and plot\n",
    "# validation error for each of the k-sets\n",
    "\n",
    "### setting up parameters\n",
    "k = 10\n",
    "lambdas = [0, 1e-5, 1e-10, 1e-15, 1e-20]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lamda in lambdas:\n",
    "    print(f'lambda: {lamda}')\n",
    "    results[lamda] = k_cross_validation(dataset, k, lamda)\n",
    "    print('=======\\n')\n",
    "\n",
    "### plotting results\n",
    "for i in range(k):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for lamda in lambdas:\n",
    "        t_error, v_error = results[lamda][i]\n",
    "        training_errors.append(t_error)\n",
    "        validation_errors.append(v_error)\n",
    "    \n",
    "    ax.plot(lambdas, training_errors, color = 'black')\n",
    "    ax.plot(lambdas, validation_errors, color = 'red')\n",
    "    \n",
    "    plt.title(f'Training Error: Red  |  Validation Error: Black')\n",
    "    plt.xlabel('lambda')\n",
    "    plt.ylabel('error')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17cb0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent for ridge regression\n",
    "# and k_cross_validation\n",
    "\n",
    "### setting up hyperparameters\n",
    "num_iterations = int(3e5)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "def ridgereg(X, Y, lamda, num_iterations = num_iterations):\n",
    "    w, b = np.random.rand(X.shape[1]), np.random.rand(X.shape[0])\n",
    "    \n",
    "    # gradient descent updates\n",
    "    for iteration in range(1, num_iterations+1):\n",
    "        delta = X@w + b - Y\n",
    "        temp = learning_rate * delta\n",
    "        b -= temp\n",
    "        w -= X.T @ temp + lamda * w\n",
    "        \n",
    "        if (iteration % 10000 == 0):\n",
    "            loss = np.sum(delta*delta) + np.sum(lamda * (w * w))\n",
    "            print(f'Training error on {iteration}th iteration  ==> {loss}\\t\\t', end = '\\r')\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    y = predridge(X, w, b)\n",
    "    train_error = np.sum((Y - y) ** 2)\n",
    "\n",
    "    return w, b, train_error\n",
    "\n",
    "def predridge(X, w, b):\n",
    "    return X@w + b\n",
    "\n",
    "\n",
    "def k_cross_validation(dataset, k, lamda):\n",
    "    kcv_datasets = dataset.k_split(k)\n",
    "    \n",
    "    validation_errors = []\n",
    "    train_errors = []\n",
    "    \n",
    "    for train, validation in kcv_datasets:\n",
    "        w, b, train_error = ridgereg(train.x, train.y, lamda)\n",
    "        y = predridge(validation.x, w, b)\n",
    "        validation_error = np.sum((validation.y - y) ** 2)\n",
    "        print(f'Validation error ==> {validation_error}')\n",
    "        validation_errors.append(validation_error)\n",
    "        train_errors.append(train_error)\n",
    "    \n",
    "    return train_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732b6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
